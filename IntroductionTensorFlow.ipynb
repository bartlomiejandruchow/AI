{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMBNuux3JEgdtJrtVzhW4x1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bartlomiejandruchow/AI/blob/main/IntroductionTensorFlow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ih1d2nOy7db"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "num_samples_per_class = 1000\n",
        "positive_samples = np.random.multivariate_normal(mean=[0, 3], cov=[[1, 0.5], [0.5, 1]], size=num_samples_per_class)\n",
        "negative_samples = np.random.multivariate_normal(mean=[3, 0], cov=[[1, 0.5], [0.5, 1]], size=num_samples_per_class)\n",
        "\n",
        "inputs = np.vstack((positive_samples, negative_samples)).astype(np.float32)\n",
        "\n",
        "targets = np.vstack((np.zeros((num_samples_per_class, 1),dtype=\"float32\"),np.ones((num_samples_per_class, 1),dtype=\"float32\")))\n",
        "\n",
        "\n",
        "input_dim = 2\n",
        "output_dim = 1\n",
        "\n",
        "W = tf.Variable(initial_value = tf.random.uniform(shape=(input_dim, output_dim)))\n",
        "b = tf.Variable(initial_value = tf.zeros(shape = (output_dim)))\n",
        "\n",
        "def model(inputs, W, b):\n",
        "  return tf.matmul(inputs, W) + b\n",
        "\n",
        "def mean_squared_error(targets, predictions):\n",
        "  per_sample_losses = tf.square(targets - predictions)\n",
        "  return tf.reduce_mean(per_sample_losses)\n",
        "\n",
        "learning_rate = 0.1\n",
        "\n",
        "def training_step(inputs, targets, W, b):\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = model(inputs, W, b)\n",
        "    loss = mean_squared_error(targets, predictions)\n",
        "  grad_loss_wrt_W, grad_loss_wrt_b = tape.gradient(loss, [W, b])\n",
        "  W.assign_sub(grad_loss_wrt_W * learning_rate)\n",
        "  b.assign_sub(grad_loss_wrt_b * learning_rate)\n",
        "  return loss\n",
        "\n",
        "for step in range(100):\n",
        "  loss = training_step(inputs, targets, W, b)\n",
        "  print(f'Loss at step {step} : {loss:.4f}')\n",
        "\n",
        "x = np.linspace(-1, 4, 100)\n",
        "y = -W[0]/W[1]*x + (0.5 - b)/W[1]\n",
        "plt.plot(x, y, '-r')\n",
        "plt.scatter(inputs[:,0], inputs[:,1], c = targets[:,0])\n",
        "#plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#PyTorch\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "num_samples_per_class = 1000\n",
        "positive_samples = np.random.multivariate_normal(mean=[0, 3], cov=[[1, 0.5], [0.5, 1]], size=num_samples_per_class)\n",
        "negative_samples = np.random.multivariate_normal(mean=[3, 0], cov=[[1, 0.5], [0.5, 1]], size=num_samples_per_class)\n",
        "\n",
        "inputs = np.vstack((positive_samples, negative_samples)).astype(np.float32)\n",
        "\n",
        "targets = np.vstack((np.zeros((num_samples_per_class, 1),dtype=\"float32\"),np.ones((num_samples_per_class, 1),dtype=\"float32\")))\n",
        "\n",
        "input_dim = 2\n",
        "output_dim = 1\n",
        "\n",
        "W = torch.rand(input_dim, output_dim, requires_grad=True)\n",
        "b = torch.zeros(output_dim, requires_grad=True)\n",
        "\n",
        "def model(inputs, W, b):\n",
        "  return torch.matmul(inputs ,W) + b\n",
        "\n",
        "def mean_squared_error(targets, predictions):\n",
        "  per_sample_losses = torch.square(targets - predictions)\n",
        "  return torch.mean(per_sample_losses)\n",
        "\n",
        "learning_rate = 0.1\n",
        "\n",
        "def training_step(inputs, targets, W, b):\n",
        "  predictions = model(inputs, W, b)\n",
        "  loss = mean_squared_error(targets, predictions)\n",
        "  loss.backword()\n",
        "  grad_loss_wrt_W, grad_loss_wrt_b = W.grad, b.grad\n",
        "  with torch.no_grad():\n",
        "    W -= grad_loss_wrt_W * learning_rate\n",
        "    b -= grad_loss_wrt_b * learning_rate\n",
        "\n",
        "  return loss\n",
        "\n",
        "for step in range(100):\n",
        "  loss = training_step(inputs, targets, W, b)\n",
        "  print(f'Loss at step {step} : {loss:.4f}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "b7eUzPooRRXF",
        "outputId": "026d1b1a-2a89-4cd5-838f-a3516c904894"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "matmul(): argument 'input' (position 1) must be Tensor, not numpy.ndarray",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1704853933.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m   \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Loss at step {step} : {loss:.4f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1704853933.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(inputs, targets, W, b)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m   \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m   \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m   \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackword\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1704853933.py\u001b[0m in \u001b[0;36mmodel\u001b[0;34m(inputs, W, b)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: matmul(): argument 'input' (position 1) must be Tensor, not numpy.ndarray"
          ]
        }
      ]
    }
  ]
}