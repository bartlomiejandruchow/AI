{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPR2WiWmkztk0tUbuDzVmFF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bartlomiejandruchow/AI/blob/main/IntroductionTensorFlow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ih1d2nOy7db"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "num_samples_per_class = 1000\n",
        "positive_samples = np.random.multivariate_normal(mean=[0, 3], cov=[[1, 0.5], [0.5, 1]], size=num_samples_per_class)\n",
        "negative_samples = np.random.multivariate_normal(mean=[3, 0], cov=[[1, 0.5], [0.5, 1]], size=num_samples_per_class)\n",
        "\n",
        "inputs = np.vstack((positive_samples, negative_samples)).astype(np.float32)\n",
        "\n",
        "targets = np.vstack((np.zeros((num_samples_per_class, 1),dtype=\"float32\"),np.ones((num_samples_per_class, 1),dtype=\"float32\")))\n",
        "\n",
        "\n",
        "input_dim = 2\n",
        "output_dim = 1\n",
        "\n",
        "W = tf.Variable(initial_value = tf.random.uniform(shape=(input_dim, output_dim)))\n",
        "b = tf.Variable(initial_value = tf.zeros(shape = (output_dim)))\n",
        "\n",
        "def model(inputs, W, b):\n",
        "  return tf.matmul(inputs, W) + b\n",
        "\n",
        "def mean_squared_error(targets, predictions):\n",
        "  per_sample_losses = tf.square(targets - predictions)\n",
        "  return tf.reduce_mean(per_sample_losses)\n",
        "\n",
        "learning_rate = 0.1\n",
        "\n",
        "def training_step(inputs, targets, W, b):\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = model(inputs, W, b)\n",
        "    loss = mean_squared_error(targets, predictions)\n",
        "  grad_loss_wrt_W, grad_loss_wrt_b = tape.gradient(loss, [W, b])\n",
        "  W.assign_sub(grad_loss_wrt_W * learning_rate)\n",
        "  b.assign_sub(grad_loss_wrt_b * learning_rate)\n",
        "  return loss\n",
        "\n",
        "for step in range(100):\n",
        "  loss = training_step(inputs, targets, W, b)\n",
        "  print(f'Loss at step {step} : {loss:.4f}')\n",
        "\n",
        "x = np.linspace(-1, 4, 100)\n",
        "y = -W[0]/W[1]*x + (0.5 - b)/W[1]\n",
        "plt.plot(x, y, '-r')\n",
        "plt.scatter(inputs[:,0], inputs[:,1], c = targets[:,0])\n",
        "#plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#PyTorch\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "num_samples_per_class = 1000\n",
        "positive_samples = np.random.multivariate_normal(mean=[0, 3], cov=[[1, 0.5], [0.5, 1]], size=num_samples_per_class)\n",
        "negative_samples = np.random.multivariate_normal(mean=[3, 0], cov=[[1, 0.5], [0.5, 1]], size=num_samples_per_class)\n",
        "\n",
        "inputs = np.vstack((positive_samples, negative_samples)).astype(np.float32)\n",
        "inputs = torch.from_numpy(inputs)\n",
        "\n",
        "targets = np.vstack((np.zeros((num_samples_per_class, 1),dtype=\"float32\"),np.ones((num_samples_per_class, 1),dtype=\"float32\")))\n",
        "targets = torch.from_numpy(targets)\n",
        "\n",
        "input_dim = 2\n",
        "output_dim = 1\n",
        "\n",
        "W = torch.rand(input_dim, output_dim, requires_grad=True)\n",
        "b = torch.zeros(output_dim, requires_grad=True)\n",
        "\n",
        "class LinearModel(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.W = torch.nn.Parameter(torch.rand(input_dim, output_dim))\n",
        "    self.b = torch.nn.Parameter(torch.zeros(output_dim))\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    return torch.matmul(inputs, self.W) + self.b\n",
        "\n",
        "model = LinearModel()\n",
        "\n",
        "def model(inputs, W, b):\n",
        "  return torch.matmul(inputs ,W) + b\n",
        "\n",
        "def mean_squared_error(targets, predictions):\n",
        "  per_sample_losses = torch.square(targets - predictions)\n",
        "  return torch.mean(per_sample_losses)\n",
        "\n",
        "learning_rate = 0.1\n",
        "\n",
        "def training_step(inputs, targets, W, b):\n",
        "  predictions = model(inputs, W, b)\n",
        "  loss = mean_squared_error(targets, predictions)\n",
        "  loss.backward()\n",
        "  grad_loss_wrt_W, grad_loss_wrt_b = W.grad, b.grad\n",
        "  with torch.no_grad():\n",
        "    W -= grad_loss_wrt_W * learning_rate\n",
        "    b -= grad_loss_wrt_b * learning_rate\n",
        "  W.grad = None\n",
        "  b.grad = None\n",
        "  return loss\n",
        "\n",
        "for step in range(100):\n",
        "  loss = training_step(inputs, targets, W, b)\n",
        "  print(f'Loss at step {step} : {loss:.4f}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7eUzPooRRXF",
        "outputId": "050e1c45-9bca-4a6e-aeac-07036ff62d4b"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss at step 0 : 2.2865\n",
            "Loss at step 1 : 0.3485\n",
            "Loss at step 2 : 0.1455\n",
            "Loss at step 3 : 0.1148\n",
            "Loss at step 4 : 0.1047\n",
            "Loss at step 5 : 0.0975\n",
            "Loss at step 6 : 0.0912\n",
            "Loss at step 7 : 0.0854\n",
            "Loss at step 8 : 0.0802\n",
            "Loss at step 9 : 0.0753\n",
            "Loss at step 10 : 0.0710\n",
            "Loss at step 11 : 0.0669\n",
            "Loss at step 12 : 0.0633\n",
            "Loss at step 13 : 0.0599\n",
            "Loss at step 14 : 0.0569\n",
            "Loss at step 15 : 0.0541\n",
            "Loss at step 16 : 0.0515\n",
            "Loss at step 17 : 0.0492\n",
            "Loss at step 18 : 0.0470\n",
            "Loss at step 19 : 0.0451\n",
            "Loss at step 20 : 0.0433\n",
            "Loss at step 21 : 0.0417\n",
            "Loss at step 22 : 0.0402\n",
            "Loss at step 23 : 0.0388\n",
            "Loss at step 24 : 0.0376\n",
            "Loss at step 25 : 0.0364\n",
            "Loss at step 26 : 0.0354\n",
            "Loss at step 27 : 0.0344\n",
            "Loss at step 28 : 0.0336\n",
            "Loss at step 29 : 0.0328\n",
            "Loss at step 30 : 0.0320\n",
            "Loss at step 31 : 0.0314\n",
            "Loss at step 32 : 0.0308\n",
            "Loss at step 33 : 0.0302\n",
            "Loss at step 34 : 0.0297\n",
            "Loss at step 35 : 0.0293\n",
            "Loss at step 36 : 0.0288\n",
            "Loss at step 37 : 0.0284\n",
            "Loss at step 38 : 0.0281\n",
            "Loss at step 39 : 0.0278\n",
            "Loss at step 40 : 0.0275\n",
            "Loss at step 41 : 0.0272\n",
            "Loss at step 42 : 0.0270\n",
            "Loss at step 43 : 0.0267\n",
            "Loss at step 44 : 0.0265\n",
            "Loss at step 45 : 0.0263\n",
            "Loss at step 46 : 0.0262\n",
            "Loss at step 47 : 0.0260\n",
            "Loss at step 48 : 0.0259\n",
            "Loss at step 49 : 0.0257\n",
            "Loss at step 50 : 0.0256\n",
            "Loss at step 51 : 0.0255\n",
            "Loss at step 52 : 0.0254\n",
            "Loss at step 53 : 0.0253\n",
            "Loss at step 54 : 0.0252\n",
            "Loss at step 55 : 0.0252\n",
            "Loss at step 56 : 0.0251\n",
            "Loss at step 57 : 0.0250\n",
            "Loss at step 58 : 0.0250\n",
            "Loss at step 59 : 0.0249\n",
            "Loss at step 60 : 0.0249\n",
            "Loss at step 61 : 0.0248\n",
            "Loss at step 62 : 0.0248\n",
            "Loss at step 63 : 0.0248\n",
            "Loss at step 64 : 0.0247\n",
            "Loss at step 65 : 0.0247\n",
            "Loss at step 66 : 0.0247\n",
            "Loss at step 67 : 0.0246\n",
            "Loss at step 68 : 0.0246\n",
            "Loss at step 69 : 0.0246\n",
            "Loss at step 70 : 0.0246\n",
            "Loss at step 71 : 0.0246\n",
            "Loss at step 72 : 0.0245\n",
            "Loss at step 73 : 0.0245\n",
            "Loss at step 74 : 0.0245\n",
            "Loss at step 75 : 0.0245\n",
            "Loss at step 76 : 0.0245\n",
            "Loss at step 77 : 0.0245\n",
            "Loss at step 78 : 0.0245\n",
            "Loss at step 79 : 0.0245\n",
            "Loss at step 80 : 0.0244\n",
            "Loss at step 81 : 0.0244\n",
            "Loss at step 82 : 0.0244\n",
            "Loss at step 83 : 0.0244\n",
            "Loss at step 84 : 0.0244\n",
            "Loss at step 85 : 0.0244\n",
            "Loss at step 86 : 0.0244\n",
            "Loss at step 87 : 0.0244\n",
            "Loss at step 88 : 0.0244\n",
            "Loss at step 89 : 0.0244\n",
            "Loss at step 90 : 0.0244\n",
            "Loss at step 91 : 0.0244\n",
            "Loss at step 92 : 0.0244\n",
            "Loss at step 93 : 0.0244\n",
            "Loss at step 94 : 0.0244\n",
            "Loss at step 95 : 0.0244\n",
            "Loss at step 96 : 0.0244\n",
            "Loss at step 97 : 0.0244\n",
            "Loss at step 98 : 0.0244\n",
            "Loss at step 99 : 0.0244\n"
          ]
        }
      ]
    }
  ]
}