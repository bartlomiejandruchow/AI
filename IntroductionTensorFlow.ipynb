{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNz1I5wNUdBmjkEiMO6HSd0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bartlomiejandruchow/AI/blob/main/IntroductionTensorFlow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ih1d2nOy7db"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "num_samples_per_class = 1000\n",
        "positive_samples = np.random.multivariate_normal(mean=[0, 3], cov=[[1, 0.5], [0.5, 1]], size=num_samples_per_class)\n",
        "negative_samples = np.random.multivariate_normal(mean=[3, 0], cov=[[1, 0.5], [0.5, 1]], size=num_samples_per_class)\n",
        "\n",
        "inputs = np.vstack((positive_samples, negative_samples)).astype(np.float32)\n",
        "\n",
        "targets = np.vstack((np.zeros((num_samples_per_class, 1),dtype=\"float32\"),np.ones((num_samples_per_class, 1),dtype=\"float32\")))\n",
        "\n",
        "\n",
        "input_dim = 2\n",
        "output_dim = 1\n",
        "\n",
        "W = tf.Variable(initial_value = tf.random.uniform(shape=(input_dim, output_dim)))\n",
        "b = tf.Variable(initial_value = tf.zeros(shape = (output_dim)))\n",
        "\n",
        "def model(inputs, W, b):\n",
        "  return tf.matmul(inputs, W) + b\n",
        "\n",
        "def mean_squared_error(targets, predictions):\n",
        "  per_sample_losses = tf.square(targets - predictions)\n",
        "  return tf.reduce_mean(per_sample_losses)\n",
        "\n",
        "learning_rate = 0.1\n",
        "\n",
        "def training_step(inputs, targets, W, b):\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = model(inputs, W, b)\n",
        "    loss = mean_squared_error(targets, predictions)\n",
        "  grad_loss_wrt_W, grad_loss_wrt_b = tape.gradient(loss, [W, b])\n",
        "  W.assign_sub(grad_loss_wrt_W * learning_rate)\n",
        "  b.assign_sub(grad_loss_wrt_b * learning_rate)\n",
        "  return loss\n",
        "\n",
        "for step in range(100):\n",
        "  loss = training_step(inputs, targets, W, b)\n",
        "  print(f'Loss at step {step} : {loss:.4f}')\n",
        "\n",
        "x = np.linspace(-1, 4, 100)\n",
        "y = -W[0]/W[1]*x + (0.5 - b)/W[1]\n",
        "plt.plot(x, y, '-r')\n",
        "plt.scatter(inputs[:,0], inputs[:,1], c = targets[:,0])\n",
        "#plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#PyTorch\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "num_samples_per_class = 1000\n",
        "positive_samples = np.random.multivariate_normal(mean=[0, 3], cov=[[1, 0.5], [0.5, 1]], size=num_samples_per_class)\n",
        "negative_samples = np.random.multivariate_normal(mean=[3, 0], cov=[[1, 0.5], [0.5, 1]], size=num_samples_per_class)\n",
        "\n",
        "inputs = np.vstack((positive_samples, negative_samples)).astype(np.float32)\n",
        "inputs = torch.from_numpy(inputs)\n",
        "\n",
        "targets = np.vstack((np.zeros((num_samples_per_class, 1),dtype=\"float32\"),np.ones((num_samples_per_class, 1),dtype=\"float32\")))\n",
        "targets = torch.from_numpy(targets)\n",
        "\n",
        "input_dim = 2\n",
        "output_dim = 1\n",
        "\n",
        "W = torch.rand(input_dim, output_dim, requires_grad=True)\n",
        "b = torch.zeros(output_dim, requires_grad=True)\n",
        "\n",
        "def model(inputs, W, b):\n",
        "  return torch.matmul(inputs ,W) + b\n",
        "\n",
        "def mean_squared_error(targets, predictions):\n",
        "  per_sample_losses = torch.square(targets - predictions)\n",
        "  return torch.mean(per_sample_losses)\n",
        "\n",
        "learning_rate = 0.1\n",
        "\n",
        "def training_step(inputs, targets, W, b):\n",
        "  predictions = model(inputs, W, b)\n",
        "  loss = mean_squared_error(targets, predictions)\n",
        "  loss.backward()\n",
        "  grad_loss_wrt_W, grad_loss_wrt_b = W.grad, b.grad\n",
        "  with torch.no_grad():\n",
        "    W -= grad_loss_wrt_W * learning_rate\n",
        "    b -= grad_loss_wrt_b * learning_rate\n",
        "  W.grad = None\n",
        "  b.grad = None\n",
        "  return loss\n",
        "\n",
        "for step in range(200):\n",
        "  loss = training_step(inputs, targets, W, b)\n",
        "  print(f'Loss at step {step} : {loss:.4f}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7eUzPooRRXF",
        "outputId": "627539f5-a1eb-4e60-c78f-2fb6b88dbfbb"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss at step 0 : 4.8908\n",
            "Loss at step 1 : 0.6591\n",
            "Loss at step 2 : 6.7213\n",
            "Loss at step 3 : 1.8072\n",
            "Loss at step 4 : 3.5935\n",
            "Loss at step 5 : 4.8221\n",
            "Loss at step 6 : 1.6106\n",
            "Loss at step 7 : 5.5870\n",
            "Loss at step 8 : 1.6561\n",
            "Loss at step 9 : 4.6172\n",
            "Loss at step 10 : 4.0122\n",
            "Loss at step 11 : 1.3570\n",
            "Loss at step 12 : 6.8933\n",
            "Loss at step 13 : 0.9632\n",
            "Loss at step 14 : 4.2553\n",
            "Loss at step 15 : 5.4650\n",
            "Loss at step 16 : 0.4882\n",
            "Loss at step 17 : 6.4482\n",
            "Loss at step 18 : 2.2634\n",
            "Loss at step 19 : 3.2371\n",
            "Loss at step 20 : 4.9724\n",
            "Loss at step 21 : 1.5707\n",
            "Loss at step 22 : 5.6009\n",
            "Loss at step 23 : 1.7247\n",
            "Loss at step 24 : 4.3445\n",
            "Loss at step 25 : 4.4821\n",
            "Loss at step 26 : 0.9426\n",
            "Loss at step 27 : 6.9429\n",
            "Loss at step 28 : 1.3906\n",
            "Loss at step 29 : 3.5933\n",
            "Loss at step 30 : 5.9483\n",
            "Loss at step 31 : 0.4501\n",
            "Loss at step 32 : 6.1005\n",
            "Loss at step 33 : 2.6998\n",
            "Loss at step 34 : 2.9456\n",
            "Loss at step 35 : 5.0851\n",
            "Loss at step 36 : 1.5232\n",
            "Loss at step 37 : 5.6214\n",
            "Loss at step 38 : 1.8358\n",
            "Loss at step 39 : 3.9974\n",
            "Loss at step 40 : 4.9837\n",
            "Loss at step 41 : 0.5951\n",
            "Loss at step 42 : 6.8572\n",
            "Loss at step 43 : 1.9216\n",
            "Loss at step 44 : 2.9415\n",
            "Loss at step 45 : 6.3185\n",
            "Loss at step 46 : 0.5350\n",
            "Loss at step 47 : 5.7074\n",
            "Loss at step 48 : 3.0964\n",
            "Loss at step 49 : 2.7138\n",
            "Loss at step 50 : 5.1786\n",
            "Loss at step 51 : 1.4609\n",
            "Loss at step 52 : 5.6347\n",
            "Loss at step 53 : 2.0078\n",
            "Loss at step 54 : 3.5772\n",
            "Loss at step 55 : 5.4910\n",
            "Loss at step 56 : 0.3431\n",
            "Loss at step 57 : 6.6326\n",
            "Loss at step 58 : 2.5283\n",
            "Loss at step 59 : 2.3353\n",
            "Loss at step 60 : 6.5629\n",
            "Loss at step 61 : 0.7244\n",
            "Loss at step 62 : 5.2981\n",
            "Loss at step 63 : 3.4413\n",
            "Loss at step 64 : 2.5299\n",
            "Loss at step 65 : 5.2699\n",
            "Loss at step 66 : 1.3840\n",
            "Loss at step 67 : 5.6225\n",
            "Loss at step 68 : 2.2551\n",
            "Loss at step 69 : 3.0940\n",
            "Loss at step 70 : 5.9735\n",
            "Loss at step 71 : 0.2105\n",
            "Loss at step 72 : 6.2762\n",
            "Loss at step 73 : 3.1774\n",
            "Loss at step 74 : 1.8061\n",
            "Loss at step 75 : 6.6788\n",
            "Loss at step 76 : 0.9940\n",
            "Loss at step 77 : 4.8980\n",
            "Loss at step 78 : 3.7308\n",
            "Loss at step 79 : 2.3777\n",
            "Loss at step 80 : 5.3718\n",
            "Loss at step 81 : 1.3001\n",
            "Loss at step 82 : 5.5645\n",
            "Loss at step 83 : 2.5860\n",
            "Loss at step 84 : 2.5664\n",
            "Loss at step 85 : 6.3990\n",
            "Loss at step 86 : 0.2133\n",
            "Loss at step 87 : 5.8048\n",
            "Loss at step 88 : 3.8323\n",
            "Loss at step 89 : 1.3780\n",
            "Loss at step 90 : 6.6732\n",
            "Loss at step 91 : 1.3162\n",
            "Loss at step 92 : 4.5271\n",
            "Loss at step 93 : 3.9690\n",
            "Loss at step 94 : 2.2397\n",
            "Loss at step 95 : 5.4915\n",
            "Loss at step 96 : 1.2235\n",
            "Loss at step 97 : 5.4409\n",
            "Loss at step 98 : 3.0009\n",
            "Loss at step 99 : 2.0201\n",
            "Loss at step 100 : 6.7366\n",
            "Loss at step 101 : 0.3582\n",
            "Loss at step 102 : 5.2440\n",
            "Loss at step 103 : 4.4573\n",
            "Loss at step 104 : 1.0663\n",
            "Loss at step 105 : 6.5618\n",
            "Loss at step 106 : 1.6633\n",
            "Loss at step 107 : 4.1980\n",
            "Loss at step 108 : 4.1669\n",
            "Loss at step 109 : 2.0994\n",
            "Loss at step 110 : 5.6283\n",
            "Loss at step 111 : 1.1732\n",
            "Loss at step 112 : 5.2355\n",
            "Loss at step 113 : 3.4910\n",
            "Loss at step 114 : 1.4860\n",
            "Loss at step 115 : 6.9600\n",
            "Loss at step 116 : 0.6415\n",
            "Loss at step 117 : 4.6258\n",
            "Loss at step 118 : 5.0201\n",
            "Loss at step 119 : 0.8766\n",
            "Loss at step 120 : 6.3666\n",
            "Loss at step 121 : 2.0102\n",
            "Loss at step 122 : 3.9151\n",
            "Loss at step 123 : 4.3396\n",
            "Loss at step 124 : 1.9440\n",
            "Loss at step 125 : 5.7746\n",
            "Loss at step 126 : 1.1708\n",
            "Loss at step 127 : 4.9382\n",
            "Loss at step 128 : 4.0390\n",
            "Loss at step 129 : 0.9976\n",
            "Loss at step 130 : 7.0502\n",
            "Loss at step 131 : 1.0493\n",
            "Loss at step 132 : 3.9856\n",
            "Loss at step 133 : 5.4945\n",
            "Loss at step 134 : 0.8046\n",
            "Loss at step 135 : 6.1132\n",
            "Loss at step 136 : 2.3371\n",
            "Loss at step 137 : 3.6751\n",
            "Loss at step 138 : 4.5042\n",
            "Loss at step 139 : 1.7667\n",
            "Loss at step 140 : 5.9157\n",
            "Loss at step 141 : 1.2374\n",
            "Loss at step 142 : 4.5468\n",
            "Loss at step 143 : 4.6195\n",
            "Loss at step 144 : 0.5873\n",
            "Loss at step 145 : 6.9972\n",
            "Loss at step 146 : 1.5584\n",
            "Loss at step 147 : 3.3594\n",
            "Loss at step 148 : 5.8631\n",
            "Loss at step 149 : 0.8375\n",
            "Loss at step 150 : 5.8280\n",
            "Loss at step 151 : 2.6312\n",
            "Loss at step 152 : 3.4678\n",
            "Loss at step 153 : 4.6770\n",
            "Loss at step 154 : 1.5679\n",
            "Loss at step 155 : 6.0317\n",
            "Loss at step 156 : 1.3912\n",
            "Loss at step 157 : 4.0683\n",
            "Loss at step 158 : 5.2018\n",
            "Loss at step 159 : 0.2837\n",
            "Loss at step 160 : 6.8014\n",
            "Loss at step 161 : 2.1389\n",
            "Loss at step 162 : 2.7802\n",
            "Loss at step 163 : 6.1177\n",
            "Loss at step 164 : 0.9555\n",
            "Loss at step 165 : 5.5355\n",
            "Loss at step 166 : 2.8876\n",
            "Loss at step 167 : 3.2788\n",
            "Loss at step 168 : 4.8706\n",
            "Loss at step 169 : 1.3556\n",
            "Loss at step 170 : 6.0998\n",
            "Loss at step 171 : 1.6444\n",
            "Loss at step 172 : 3.5185\n",
            "Loss at step 173 : 5.7515\n",
            "Loss at step 174 : 0.1090\n",
            "Loss at step 175 : 6.4736\n",
            "Loss at step 176 : 2.7563\n",
            "Loss at step 177 : 2.2754\n",
            "Loss at step 178 : 6.2599\n",
            "Loss at step 179 : 1.1350\n",
            "Loss at step 180 : 5.2549\n",
            "Loss at step 181 : 3.1089\n",
            "Loss at step 182 : 3.0911\n",
            "Loss at step 183 : 5.0919\n",
            "Loss at step 184 : 1.1448\n",
            "Loss at step 185 : 6.0972\n",
            "Loss at step 186 : 2.0013\n",
            "Loss at step 187 : 2.9217\n",
            "Loss at step 188 : 6.2349\n",
            "Loss at step 189 : 0.0763\n",
            "Loss at step 190 : 6.0341\n",
            "Loss at step 191 : 3.3753\n",
            "Loss at step 192 : 1.8646\n",
            "Loss at step 193 : 6.3000\n",
            "Loss at step 194 : 1.3508\n",
            "Loss at step 195 : 4.9993\n",
            "Loss at step 196 : 3.3048\n",
            "Loss at step 197 : 2.8887\n",
            "Loss at step 198 : 5.3407\n",
            "Loss at step 199 : 0.9563\n"
          ]
        }
      ]
    }
  ]
}